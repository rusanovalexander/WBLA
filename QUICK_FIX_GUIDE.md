# QUICK FIX IMPLEMENTATION GUIDE
## Credit Pack Multi-Agent PoC v3.2

**Target:** Fix the "could not parse LLM output" error and make the system production-ready in 3-5 days.

---

## CRITICAL FIX #1: Update Parsers (30 minutes)

### File: `/refactored/core/parsers.py`

**Problem:** JSON extraction fails when LLM adds preambles or uses incorrect formatting.

**Solution:**
1. **Replace the entire file** with `/home/claude/parsers_FIXED.py`
2. Or apply these specific changes:

**Change 1:** Update `safe_extract_json()` function (starts at line 221)
```python
# Add XML tag extraction at the beginning of the function (after line 234):

# NEW: Try to extract from XML tags if present
xml_pattern = r'<json_output>\s*([\s\S]*?)\s*</json_output>'
xml_match = re.search(xml_pattern, text, re.IGNORECASE)
if xml_match:
    logger.debug("Found JSON within <json_output> tags")
    text = xml_match.group(1).strip()

# NEW: Remove common preambles (after line 237):
preamble_patterns = [
    r'^(?:here\s+is|here\'s|the\s+json|output:)\s*',
    r'^(?:sure|okay|certainly)[,.]?\s*',
]
for pattern in preamble_patterns:
    cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
```

**Change 2:** Update `_try_parse_json()` function (starts at line 277)
```python
# Add two new attempts before the final return None (after attempt 3):

# Attempt 4: Fix unquoted keys
fixed3 = re.sub(
    r'(\{|,)\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:',
    r'\1"\2":',
    fixed2
)
try:
    result = json.loads(fixed3)
    logger.debug("JSON parsed after quoting unquoted keys")
    return result
except json.JSONDecodeError:
    pass

# Attempt 5: Fix missing commas
fixed4 = re.sub(r'}\s*{', '},{', fixed3)
fixed4 = re.sub(r']\s*\[', '],[', fixed4)
try:
    result = json.loads(fixed4)
    logger.debug("JSON parsed after adding missing commas")
    return result
except json.JSONDecodeError as e:
    logger.warning(
        "All JSON parse attempts failed. Last error: %s. JSON string (first 300 chars): %s",
        str(e), json_str[:300]
    )
```

---

## CRITICAL FIX #2: Update Extraction Prompts (15 minutes)

### File: `/refactored/core/orchestration.py`

**Problem:** Extraction prompts don't constrain LLM output enough.

**Solution:** Replace the two extraction prompts (lines 57-116)

**Change 1:** Replace `PROCESS_DECISION_EXTRACTION_PROMPT` (line 57)
```python
PROCESS_DECISION_EXTRACTION_PROMPT = """You are a JSON extraction assistant. Extract the process path decision from the analysis below.

## ANALYSIS TEXT
{analysis_text}

## TASK
Extract the agent's decision into EXACTLY this JSON format. Use ONLY what the agent explicitly stated.

CRITICAL FORMATTING RULES:
- You MUST output ONLY valid JSON with NO text before or after
- Do NOT include markdown code fences like ```json
- Do NOT include explanations, preambles, or any other text
- Output ONLY the JSON object between the <json_output></json_output> tags below

<json_output>
{{
  "assessment_approach": "<exact approach name the agent recommended>",
  "origination_method": "<exact origination method the agent recommended>",
  "assessment_reasoning": "<1-2 sentence summary of WHY this assessment approach>",
  "origination_reasoning": "<1-2 sentence summary of WHY this origination method>",
  "procedure_sections_cited": ["Section X.X", "Section Y.Y"],
  "confidence": "HIGH",
  "decision_found": true
}}
</json_output>

EXTRACTION RULES:
- If the agent did NOT clearly state an assessment approach, set "decision_found": false
- Do NOT invent or assume an approach ‚Äî only use what the agent explicitly recommended
- Copy the agent's EXACT wording for the approach and method names
- confidence must be exactly one of: "HIGH", "MEDIUM", "LOW"
- decision_found must be exactly: true or false (boolean)

NOW: Extract the decision from the analysis text above. Output ONLY the JSON between <json_output></json_output> tags with NO other text.
"""
```

**Change 2:** Replace `COMPLIANCE_EXTRACTION_PROMPT` (line 86)
```python
COMPLIANCE_EXTRACTION_PROMPT = """You are a JSON extraction assistant. Extract ALL compliance checks from the analysis below.

## COMPLIANCE ANALYSIS TEXT
{compliance_text}

## TASK
Extract every compliance criterion the agent checked into a JSON array.

CRITICAL FORMATTING RULES:
- You MUST output ONLY valid JSON with NO text before or after
- Do NOT include markdown code fences like ```json
- Do NOT include explanations, preambles, or any other text
- Output ONLY the JSON array between the <json_output></json_output> tags below

<json_output>
[
  {{
    "criterion": "<name of the criterion>",
    "guideline_limit": "<the limit from the Guidelines>",
    "deal_value": "<the deal's actual value>",
    "status": "PASS",
    "evidence": "<brief reasoning>",
    "reference": "<Guidelines section>",
    "severity": "MUST"
  }}
]
</json_output>

EXTRACTION RULES:
- Include EVERY criterion the agent assessed
- status must be exactly one of: "PASS", "FAIL", "REVIEW", "N/A"
- severity must be exactly one of: "MUST", "SHOULD"
- If no compliance checks found, return empty array: []

NOW: Extract ALL compliance checks. Output ONLY the JSON array between <json_output></json_output> tags with NO other text.
"""
```

---

## CRITICAL FIX #3: Add Retry Logic (20 minutes)

### File: `/refactored/core/orchestration.py`

**Problem:** Extraction fails on first attempt and doesn't retry.

**Solution:** Update the extraction functions to retry with different temperatures.

**Change 1:** Update `_extract_structured_decision()` (line 119)
```python
def _extract_structured_decision(
    analysis_text: str,
    tracer: TraceStore,
) -> dict | None:
    """
    Use a dedicated LLM call to extract structured decision with retry.
    """
    tracer.record("Extraction", "START", "Extracting structured decision")
    
    prompt = PROCESS_DECISION_EXTRACTION_PROMPT.format(
        analysis_text=analysis_text[:12000]
    )
    
    # Try up to 2 times with different temperatures
    for attempt in range(2):
        temperature = 0.0 if attempt == 0 else 0.1
        
        tracer.record("Extraction", "ATTEMPT", f"Attempt {attempt + 1}/2 (temp={temperature})")
        
        result = call_llm(prompt, MODEL_FLASH, temperature, 2000, "Extraction", tracer)
        parsed = safe_extract_json(result.text, "object")
        
        if parsed and "decision_found" in parsed:
            if parsed.get("decision_found"):
                tracer.record("Extraction", "SUCCESS", f"Found: {parsed.get('assessment_approach', '?')}")
                return parsed
            else:
                tracer.record("Extraction", "NO_DECISION", "Agent did not make clear decision")
                return parsed
    
    # Both attempts failed
    tracer.record("Extraction", "FAILED", f"Could not extract after 2 attempts. Output length: {len(result.text)}")
    logger.error("Extraction failed. Last output (first 1000 chars): %s", result.text[:1000])
    return None
```

**Change 2:** Update `_extract_compliance_checks()` (line 143)
```python
def _extract_compliance_checks(
    compliance_text: str,
    tracer: TraceStore,
) -> list[dict]:
    """
    Use a dedicated LLM call to extract compliance checks with retry.
    """
    tracer.record("Extraction", "START", "Extracting compliance checks")
    
    prompt = COMPLIANCE_EXTRACTION_PROMPT.format(
        compliance_text=compliance_text[:12000]
    )
    
    # Try up to 2 times
    for attempt in range(2):
        temperature = 0.0 if attempt == 0 else 0.1
        
        tracer.record("Extraction", "ATTEMPT", f"Attempt {attempt + 1}/2 (temp={temperature})")
        
        result = call_llm(prompt, MODEL_FLASH, temperature, 4000, "Extraction", tracer)
        parsed = safe_extract_json(result.text, "array")
        
        if parsed is not None and isinstance(parsed, list):
            tracer.record("Extraction", "SUCCESS", f"Extracted {len(parsed)} checks")
            return parsed
    
    # Both attempts failed
    tracer.record("Extraction", "FAILED", "Could not extract checks after 2 attempts")
    logger.error("Compliance extraction failed. Last output (first 1000 chars): %s", result.text[:1000])
    return []
```

---

## CRITICAL FIX #4: Add User Error Messages (15 minutes)

### File: `/refactored/ui/app.py`

**Problem:** When parsing fails, users see no helpful error message.

**Solution:** Add user-friendly error displays

**Location:** Around line 300-320 in `render_phase_analysis()` function

**Add this code block** after the analysis completes but before the "Process Path Determined" success message:

```python
# After line that sets st.session_state.decision_found

if not st.session_state.decision_found:
    st.error("""
    ‚ö†Ô∏è **Process Path Could Not Be Determined**
    
    The Process Analyst could not determine a clear assessment approach and origination method from the teaser.
    
    **Possible causes:**
    - Teaser document lacks sufficient deal information
    - Deal characteristics are ambiguous or contradictory  
    - RAG search returned no relevant Procedure sections
    - LLM output could not be parsed into structured format
    
    **What you can do:**
    1. Review the analyst's reasoning in the "Full Analysis" section above
    2. Manually select the assessment approach and origination method using the form below
    3. Upload additional documents with more deal details
    4. Retry the analysis after adding information
    
    **For technical support:** Check the Agent Activity log in the sidebar for detailed error messages.
    """)
    
    # Show manual selection form
    st.subheader("Manual Process Path Selection")
    
    col1, col2 = st.columns(2)
    
    with col1:
        manual_assessment = st.text_input(
            "Assessment Approach",
            placeholder="e.g., Proportionality Approach",
            help="Enter the assessment approach from the Procedure document"
        )
    
    with col2:
        manual_origination = st.text_input(
            "Origination Method",
            placeholder="e.g., Credit Committee Approval",
            help="Enter the origination method from the Procedure document"
        )
    
    if st.button("Confirm Manual Selection"):
        if manual_assessment and manual_origination:
            st.session_state.process_path = manual_assessment
            st.session_state.origination_method = manual_origination
            st.session_state.decision_found = True
            st.session_state.assessment_reasoning = "Manually specified by user"
            st.session_state.origination_reasoning = "Manually specified by user"
            st.success(f"‚úÖ Process path set: {manual_assessment} / {manual_origination}")
            st.rerun()
        else:
            st.warning("Please enter both assessment approach and origination method")
```

---

## CRITICAL FIX #5: Update Model Versions (5 minutes)

### File: `/refactored/config/settings.py`

**Problem:** Using preview model versions that may expire or change behavior.

**Solution:** Update lines 44-45

**Replace:**
```python
MODEL_PRO = os.getenv("MODEL_PRO", "gemini-2.5-pro-preview-05-06")
MODEL_FLASH = os.getenv("MODEL_FLASH", "gemini-2.5-flash-preview-04-17")
```

**With:**
```python
# Use stable, production-ready models
MODEL_PRO = os.getenv("MODEL_PRO", "gemini-2.0-flash-exp")
MODEL_FLASH = os.getenv("MODEL_FLASH", "gemini-2.0-flash-exp")

# Alternative: Use latest experimental if you need cutting edge features
# MODEL_PRO = os.getenv("MODEL_PRO", "gemini-exp-1206")
# MODEL_FLASH = os.getenv("MODEL_FLASH", "gemini-2.0-flash-exp")
```

---

## VERIFICATION CHECKLIST

After implementing all fixes, verify:

### ‚úÖ Step 1: Test Parsing
```bash
cd /refactored
python3 << 'EOF'
from core.parsers import safe_extract_json

# Test 1: With XML tags
test1 = '<json_output>{"decision_found": true}</json_output>'
assert safe_extract_json(test1, "object") == {"decision_found": True}

# Test 2: With preamble
test2 = 'Here is the JSON:\n{"decision_found": true}'
assert safe_extract_json(test2, "object") == {"decision_found": True}

# Test 3: With markdown
test3 = '```json\n{"decision_found": true}\n```'
assert safe_extract_json(test3, "object") == {"decision_found": True}

print("‚úÖ All parsing tests passed!")
EOF
```

### ‚úÖ Step 2: Test Full Workflow
```bash
# Start the app
streamlit run ui/app.py

# Then test:
1. Upload a sample teaser document
2. Click "Start Analysis"
3. Verify:
   - ‚úÖ No "could not parse LLM output" error
   - ‚úÖ Process path is determined OR clear error message shown
   - ‚úÖ Agent activity log shows successful extraction
   - ‚úÖ If extraction fails, manual selection form appears
```

### ‚úÖ Step 3: Check Logs
```bash
# Look for these success messages in logs:
grep "Successfully parsed JSON" logs/*.log
grep "Extraction SUCCESS" logs/*.log

# Should see minimal parse failures:
grep "Could not parse JSON" logs/*.log | wc -l
# Should be < 5% of total extraction attempts
```

---

## EXPECTED RESULTS

**Before Fixes:**
- ‚ùå Parsing success rate: ~60-70%
- ‚ùå User sees cryptic error messages
- ‚ùå Workflow blocks completely on parse failure
- ‚ùå No way to recover without restarting

**After Fixes:**
- ‚úÖ Parsing success rate: >95%
- ‚úÖ Clear, actionable error messages
- ‚úÖ Automatic retry with different temperatures
- ‚úÖ Manual override option if automation fails
- ‚úÖ Detailed logging for debugging

---

## TROUBLESHOOTING

### Issue: Still seeing parse errors after fixes

**Check 1:** Verify you updated the RIGHT files
```bash
# Check parsers.py has the new code
grep "xml_pattern" /refactored/core/parsers.py
# Should output: xml_pattern = r'<json_output>...

# Check orchestration.py has new prompts
grep "<json_output>" /refactored/core/orchestration.py
# Should output: <json_output>
```

**Check 2:** Restart the Streamlit app
```bash
# Kill existing process
pkill -f streamlit

# Start fresh
streamlit run ui/app.py
```

**Check 3:** Clear Python cache
```bash
find /refactored -type d -name __pycache__ -exec rm -rf {} +
find /refactored -name "*.pyc" -delete
```

### Issue: Different error appears

**Check:** Look at the Agent Activity log in the sidebar
- If "PARSE_FAIL" ‚Üí The JSON is still malformed
- If "NO_DECISION" ‚Üí Agent couldn't determine process path (not a bug)
- If "TIMEOUT" ‚Üí Add the timeout fix from the comprehensive audit

### Issue: Manual selection doesn't work

**Fix:** Make sure you added the manual selection form code in the RIGHT location:
- It should be INSIDE the `if not st.session_state.decision_found:` block
- It should be BEFORE the next phase transition

---

## DEPLOYMENT

### Option 1: Direct File Replacement (Fastest)
```bash
# Backup original files
cp /refactored/core/parsers.py /refactored/core/parsers.py.backup
cp /refactored/core/orchestration.py /refactored/core/orchestration.py.backup
cp /refactored/config/settings.py /refactored/config/settings.py.backup
cp /refactored/ui/app.py /refactored/ui/app.py.backup

# Copy fixed files
cp /home/claude/parsers_FIXED.py /refactored/core/parsers.py

# Manually apply other changes using your text editor
# (orchestration prompts, settings model versions, ui error messages)

# Restart
streamlit run ui/app.py
```

### Option 2: Git Branch (Recommended for Production)
```bash
cd /refactored
git checkout -b fix/parsing-errors
git add core/parsers.py core/orchestration.py config/settings.py ui/app.py
git commit -m "Fix: Improve LLM output parsing and add retry logic

- Add XML tag extraction in safe_extract_json()
- Strengthen extraction prompts with explicit formatting rules
- Add retry logic with temperature variation
- Add user-friendly error messages
- Update to stable model versions
"
git push origin fix/parsing-errors

# Create PR for review, then merge
```

---

## TIME ESTIMATE

- **Reading this guide:** 15 minutes
- **Implementing Fix #1 (parsers):** 30 minutes
- **Implementing Fix #2 (prompts):** 15 minutes
- **Implementing Fix #3 (retry):** 20 minutes
- **Implementing Fix #4 (UI errors):** 15 minutes
- **Implementing Fix #5 (models):** 5 minutes
- **Testing & verification:** 30 minutes
- **Documentation & deploy:** 30 minutes

**Total: 2.5-3 hours** for critical fixes

**With buffer for issues: 4-5 hours** (1 business day)

---

## NEXT STEPS

After implementing these critical fixes:

1. **Monitor for 24-48 hours** 
   - Track parsing success rate
   - Watch for new error patterns
   - Collect user feedback

2. **Implement High Priority Fixes** (from comprehensive audit)
   - Add timeout on LLM calls
   - Fix race conditions
   - Add input validation

3. **Add Testing** (week 2)
   - Unit tests for parsers
   - Integration tests for orchestration
   - End-to-end workflow tests

4. **Performance Optimization** (week 3)
   - Parallel tool calls
   - Document caching
   - Batch LLM calls

5. **Security Hardening** (week 4)
   - Secret Manager integration
   - Input sanitization
   - RBAC implementation

---

## SUPPORT

If you encounter issues after implementing these fixes:

1. **Check the comprehensive audit:** `/home/claude/COMPREHENSIVE_CODE_AUDIT.md`
2. **Review fixed code:** `/home/claude/parsers_FIXED.py` and `/home/claude/orchestration_EXTRACTION_FIXES.py`
3. **Look at logs:** Agent Activity sidebar + application logs
4. **Test in isolation:** Run the parsing tests in Step 1 of verification

**Critical fixes should resolve >90% of current blocking issues.**

Good luck! üöÄ
