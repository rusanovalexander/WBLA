C:\Users\GV20FI\Documents\Mode Development\AI in WB lending process\Project Omni\WBLA-feature-autonomous-agents_v28>python -m streamlit run ui/chat_app.py

  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://172.16.88.235:8501
  External URL: http://144.178.220.33:8501

Streaming LLM call failed for Writer: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}
Traceback (most recent call last):
  File "C:\Users\GV20FI\Documents\Mode Development\AI in WB lending process\Project Omni\WBLA-feature-autonomous-agents_v28\core\llm_client.py", line 546, in call_llm_streaming
    result_text = _call_gemini_streaming(
                  ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\Documents\Mode Development\AI in WB lending process\Project Omni\WBLA-feature-autonomous-agents_v28\core\llm_client.py", line 499, in _call_gemini_streaming
    for chunk in client.models.generate_content_stream(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\models.py", line 5404, in generate_content_stream
    for chunk in response:
                 ^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\models.py", line 4098, in _generate_content_stream
    for response in self._api_client.request_streamed(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\_api_client.py", line 1403, in request_streamed
    session_response = self._request(http_request, http_options, stream=True)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\_api_client.py", line 1222, in _request
    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\_api_client.py", line 1187, in _request_once
    errors.APIError.raise_for_response(response)
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}
Streaming LLM call failed for Writer: peer closed connection without sending complete message body (incomplete chunked read)
Traceback (most recent call last):
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpx\_transports\default.py", line 127, in __iter__
    for part in self._httpcore_stream:
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpcore\_sync\connection_pool.py", line 407, in __iter__
    raise exc from None
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpcore\_sync\connection_pool.py", line 403, in __iter__
    for part in self._stream:
                ^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpcore\_sync\http11.py", line 342, in __iter__
    raise exc
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpcore\_sync\http11.py", line 334, in __iter__
    for chunk in self._connection._receive_response_body(**kwargs):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpcore\_sync\http11.py", line 203, in _receive_response_body
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpcore\_sync\http11.py", line 213, in _receive_event
    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\GV20FI\Documents\Mode Development\AI in WB lending process\Project Omni\WBLA-feature-autonomous-agents_v28\core\llm_client.py", line 546, in call_llm_streaming
    result_text = _call_gemini_streaming(
                  ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\Documents\Mode Development\AI in WB lending process\Project Omni\WBLA-feature-autonomous-agents_v28\core\llm_client.py", line 499, in _call_gemini_streaming
    for chunk in client.models.generate_content_stream(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\models.py", line 5404, in generate_content_stream
    for chunk in response:
                 ^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\models.py", line 4098, in _generate_content_stream
    for response in self._api_client.request_streamed(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\_api_client.py", line 1404, in request_streamed
    for chunk in session_response.segments():
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\_api_client.py", line 283, in segments
    for chunk in self._iter_response_stream():
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\google\genai\_api_client.py", line 326, in _iter_response_stream
    for line in self.response_stream.iter_lines():
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpx\_models.py", line 929, in iter_lines
    for text in self.iter_text():
                ^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpx\_models.py", line 916, in iter_text
    for byte_content in self.iter_bytes():
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpx\_models.py", line 897, in iter_bytes
    for raw_bytes in self.iter_raw():
                     ^^^^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpx\_models.py", line 951, in iter_raw
    for raw_stream_bytes in self.stream:
                            ^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpx\_client.py", line 153, in __iter__
    for chunk in self._stream:
                 ^^^^^^^^^^^^
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpx\_transports\default.py", line 126, in __iter__
    with map_httpcore_exceptions():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "C:\Users\GV20FI\AppData\Roaming\Python\Python312\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)
